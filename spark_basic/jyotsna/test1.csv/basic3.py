from session import spark

sc=spark.sparkContext
# rdd=sc.textFile('word.txt')
# print(rdd.collect())
# print(rdd.take(2))
# print(rdd.first())
# rdd1=rdd.map(lambda x:(x,1))
# rdd1=rdd.map(lambda x:x.split(' '))
# rdd2=rdd1.map(lambda x:(x,1))
# print(rdd2.collect())
# print(rdd2.count())

#
# rdd3=rdd.flatMap(lambda x:x.split(' '))
# rdd4=rdd3.map(lambda x:(x,1))
# print(rdd4.collect())
# rdd5=rdd4.reduceByKey(lambda x,y:x+y)
# print(rdd5.collect())
# print(sorted(rdd5.collect()))
# rdd_sorted=rdd5.sortByKey(False)
# print(rdd_sorted.collect())
# # rdd_group=rdd4.groupByKey(lambda x,y:x+y)
# # print(rdd_group.collect())
# stopword=['is','on']
# rdd6=rdd3.filter(lambda x: x != 'is')
# rdd7=rdd3.filter(lambda x: x not in stopword)
# print(rdd6.collect())
# print(rdd7.collect())
# rdd8=rdd5.filter(lambda x: x[0]=='is')
# print(rdd8.collect())
# rdd9=rdd4.filter(lambda x:x[0]=='s%')
# print(rdd9.collect())
# list1=[1,2,3,4,5]
# rdd21=spark.sparkContext.parallelize(list1)
# print(rdd21.collect())
# new operations on RDD
rdd22=spark.sparkContext.textFile("blogtexts1")
print(rdd22.collect())
rdd23=rdd22.flatMap(lambda x:x.split(' '))
print(rdd23.collect())
rdd24=rdd23.map(lambda x:(x,1))
print(rdd24.collect())
rdd25=rdd24.reduceByKey(lambda x,y:x+1)
print(rdd25.collect())
rdd26=rdd25.sortByKey()
print(rdd26.collect())
rdd27=rdd26.sortByKey(False)
print(rdd27.collect())
rdd28=rdd27.sortBy(lambda x:x[1])
print(rdd28.collect())
rdd29=rdd28.sortBy(lambda x:x[1],False)
print(rdd29.collect())
print(rdd28.first())
print(rdd28.take(3))
print(rdd29.first())
print(rdd29.take(3))
rdd30=rdd29.keys()
print(rdd30.collect())
rdd31=rdd29.values()
print(rdd31.collect())
rdd32=rdd25.filter(lambda x:x[1] % 2 == 0)
print(rdd32.collect())
rdd33=rdd25.filter(lambda x:x[1] % 2 == 1)
print(rdd33.collect())
rdd34=rdd25.filter(lambda x:x[1] >2)
print(rdd34.collect())
rdd35=rdd25.filter(lambda x: x[0].startswith('d'))
print(rdd35.collect())
rdd36=rdd25.filter(lambda x: x[0].endswith('n'))
print(rdd36.collect())
rdd37=rdd25.filter(lambda x: x[0].find('ll')!=-1)
print(rdd37.collect())
rdd25.saveAsTextFile('RDD operations')
display(spark.read.text('RDD operations'))