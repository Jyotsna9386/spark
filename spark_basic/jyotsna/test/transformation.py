from session import spark
sc=spark.sparkContext
rdd1=sc.textFile('employee1')
print(rdd1.collect())
rdd_new=rdd1.map(lambda x:x.split(','))
print(rdd_new)
print(rdd1.collect())
print(rdd1.take(2))
print(rdd1.first())
print(rdd1.count())
rdd2=rdd1.map(lambda x:(x,1))
print(rdd2.collect())
print(rdd2.count())
rdd3=rdd2.reduceByKey(lambda x,y:x+1)
print(rdd3.collect())
group=rdd2.groupByKey(lambda x,y:(x+1))
print(group.collect())
# rdd6=rdd1.flatMap(lambda x:x.split(','))
# print(rdd6.collect())
# rdd7=rdd6.map(lambda x:(x,1))
# print(rdd7.collect())
# rdd8=rdd7.reduceByKey(lambda x,y:(x+1))
# print(rdd8.collect())
# rdd_sort_key=rdd8.sortBy(lambda x:x[1])
# print(rdd_sort_key.collect())
# rdd_key=rdd8.values()
# print(rdd_key.collect())
# rdd9=rdd8.filter(lambda x:x[0].startswith('H'))
# print(rdd9.collect())
# rdd9=rdd8.filter(lambda x:x[0].endswith('a'))
# print(rdd9.collect())
# rdd9=rdd8.filter(lambda x:x[0].find('cc')!=-1)
# print(rdd9.collect())
# rdd9=rdd8.filter(lambda x:x[0]%2==0)
# print(rdd9.collect())
# rdd9=rdd8.filter(lambda x:x[0]%2==1)
# print(rdd9.collect())
# rdd9=rdd8.filter(lambda x:x[0]>1)
# print(rdd9.collect())
